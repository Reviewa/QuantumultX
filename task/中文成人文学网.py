import requests
from bs4 import BeautifulSoup
import re
import time
import os
import random
import unicodedata

# æ›´æ–°è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ä»¥ç»•è¿‡æ½œåœ¨é™åˆ¶
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36",
    "Referer": "https://book.xbookcn.net/",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
    "Connection": "keep-alive"
}

def safe_filename(name, ext=".txt"):
    """å°†ä¹¦åè½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
    name = unicodedata.normalize("NFKD", name)
    name = re.sub(r"[^\w\s-]", "", name)
    name = name.strip().replace(" ", "_")
    return name + ext if name else "novel" + ext

def get_search_url(bookname):
    """æ„é€ æœç´¢URLï¼Œå¹¶è®¾ç½®max-results=500ä»¥æŠ“å–æ›´å¤šç« èŠ‚"""
    encoded_name = re.sub(r"\s+", "+", bookname.strip())
    return f"https://book.xbookcn.net/search/label/{encoded_name}?max-results=500"

def get_next_page_url(soup):
    """æå–ä¸‹ä¸€é¡µURL"""
    next_btn = soup.find("a", attrs={"rel": "next"})
    return next_btn["href"] if next_btn else None

def get_post_links(base_url, bookname, max_pages=100):
    """æŠ“å–ç›®å½•é¡µä¸­ä¸ä¹¦åç›¸å…³çš„æ–‡ç« é“¾æ¥ï¼Œæ”¯æŒå¤šé¡µ"""
    links = []
    next_page = base_url
    page_count = 0

    while next_page and page_count < max_pages:
        print(f"ğŸ“„ æŠ“å–ç›®å½•é¡µ: {next_page}")
        try:
            resp = requests.get(next_page, headers=headers, timeout=10)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding
            soup = BeautifulSoup(resp.text, "html.parser")

            for a in soup.find_all("a", href=True):
                href = a["href"]
                title = a.get_text(strip=True)
                if href and title and "book.xbookcn.net" in href and len(title) < 50:
                    if bookname.lower() in title.lower():
                        links.append((title, href))

            next_page = get_next_page_url(soup)
            page_count += 1
            time.sleep(random.uniform(1, 3))
        except requests.RequestException as e:
            print(f"âŒ ç›®å½•é¡µè¯·æ±‚å¤±è´¥: {e}")
            break

    print(f"âœ… å…±è·å–åˆ°ä¸ '{bookname}' ç›¸å…³çš„ {len(links)} ç¯‡æ–‡ç« ")
    return links

def extract_text(url):
    """æå–æ–‡ç« æ­£æ–‡ï¼Œæ’é™¤å¯¼èˆªå’Œæ¨èå†…å®¹"""
    print(f"ğŸ” æå–æ­£æ–‡: {url}")
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        resp.encoding = resp.apparent_encoding
        soup = BeautifulSoup(resp.text, "html.parser")

        # è°ƒè¯•ï¼šæ‰“å°é¡µé¢æ ‡é¢˜
        print(f"é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")

        # å°è¯•æŸ¥æ‰¾æ­£æ–‡å®¹å™¨
        content_div = soup.find("div", class_=re.compile(r"post-body|content|entry"))
        if content_div:
            for tag in content_div.find_all(["a", "nav"]):
                tag.decompose()  # ç§»é™¤<a>å’Œ<nav>æ ‡ç­¾
            text = content_div.get_text(" ", strip=True)
            print(f"âœ… æ‰¾åˆ°æ­£æ–‡å—: {len(text)} å­—ç¬¦")
        else:
            # å¤‡ç”¨ï¼šæŸ¥æ‰¾ä¸­æ–‡å­—ç¬¦æœ€å¤šçš„å—ï¼Œæ’é™¤å¯¼èˆª
            candidates = soup.find_all(["div", "article", "section"])
            text = max((tag.get_text(" ", strip=True) for tag in candidates if tag.get_text() and not tag.find("nav")),
                      key=lambda x: len(re.findall(r"[\u4e00-\u9fff]", x)), default="")
            print(f"âš ï¸ ä½¿ç”¨å¤‡ç”¨é€»è¾‘ï¼Œå­—ç¬¦æ•°: {len(text)}")

        # æ¸…ç†å¤šä½™ç©ºç™½å’Œè¿‡æ»¤æ— å…³å…³é”®è¯
        text = re.sub(r"\s+", " ", text).strip()
        text = re.sub(r"(ä¸Šä¸€é¡µ|ä¸‹ä¸€é¡µ|ä¸»é¡µ|åˆ†ç±»|ä½œå®¶ä¸“æ |å…¨éƒ¨å°è¯´|åˆ—è¡¨|\w+å°è¯´)", "", text, flags=re.IGNORECASE)
        return text if text and len(re.findall(r"[\u4e00-\u9fff]", text)) > 10 else "æ­£æ–‡æå–å¤±è´¥"
    except requests.RequestException as e:
        print(f"âŒ æ­£æ–‡è¯·æ±‚å¤±è´¥: {e}")
        return f"æå–å¤±è´¥: {e}"

def download_book(bookname):
    """ä¸‹è½½æŒ‡å®šä¹¦åçš„æ•´æœ¬ä¹¦"""
    safe_name = safe_filename(bookname)
    save_dir = os.path.expanduser("~/Documents")
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    filepath = os.path.join(save_dir, safe_name)

    # æ„é€ æœç´¢URLå¹¶è·å–ç›¸å…³é“¾æ¥
    toc_url = get_search_url(bookname)
    posts = get_post_links(toc_url, bookname)
    if not posts:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°ä¸ '{bookname}' ç›¸å…³çš„æ–‡ç« é“¾æ¥")
        return

    with open(filepath, "w", encoding="utf-8") as f:
        for i, (title, link) in enumerate(posts, 1):
            print(f"ğŸ“– ä¸‹è½½ {i}/{len(posts)}: {title}")
            try:
                content = extract_text(link)
                if "æå–å¤±è´¥" not in content:
                    f.write(f"\n\n=== ç¬¬{i}ç«  {title} ===\n\n")
                    f.write(content + "\n")
                else:
                    print(f"âš ï¸ è·³è¿‡ç©ºå†…å®¹: {title}")
            except Exception as e:
                print(f"âŒ ç« èŠ‚ä¸‹è½½å¤±è´¥: {title}, é”™è¯¯: {e}")
            time.sleep(random.uniform(1, 2))

    print(f"\nğŸ‰ '{bookname}' ä¸‹è½½å®Œæˆï¼Œä¿å­˜ä¸º: {filepath}")

if __name__ == "__main__":
    # ç”¨æˆ·è¾“å…¥ä¹¦å
    bookname = input("è¯·è¾“å…¥è¦ä¸‹è½½çš„ä¹¦åï¼ˆä¾‹å¦‚ 'å°æ‘æ˜¥è‰²'ï¼‰ï¼š")
    download_book(bookname)